{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import sampler, TensorDataset, Dataset\n",
    "\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import random\n",
    "import scipy\n",
    "import time\n",
    "import cv2\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.metrics import mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA GENERATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = 299\n",
    "h = 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_crop(img, w, h):\n",
    "    img_h = img.shape[0]\n",
    "    img_w = img.shape[1]\n",
    "    start_h, start_w = random.randint(0, int(img_h - h)), random.randint(0, int(img_w - w))\n",
    "    return img[start_h:start_h+h, start_w:start_w+w, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image pre-process\n",
    "# mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]\n",
    "def img_prepro(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (w,h))\n",
    "#     img = img_crop(img, w, h)\n",
    "    img = img/255.\n",
    "    img[:, :, 0] = (img[:, :, 0] - 0.485)/0.229\n",
    "    img[:, :, 1] = (img[:, :, 1] - 0.456)/0.224\n",
    "    img[:, :, 2] = (img[:, :, 2] - 0.406)/0.225\n",
    "    img = np.swapaxes(np.swapaxes(img, 0, 1), 0, 2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generation(train_id):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in train_id:\n",
    "        path = df_r[df_r['uid_date'] == i]['path']\n",
    "        for p in path:\n",
    "            try:\n",
    "                X.append(img_prepro(p))\n",
    "                Y.append(df_r[df_r['path'] == p]['target'].values[0])\n",
    "            except:\n",
    "                print(p)\n",
    "    X = np.asarray(X)\n",
    "    Y = np.asarray(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9900, 15)\n",
      "(9900, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>path</th>\n",
       "      <th>type</th>\n",
       "      <th>uid</th>\n",
       "      <th>chtno</th>\n",
       "      <th>name</th>\n",
       "      <th>contentdate</th>\n",
       "      <th>year</th>\n",
       "      <th>uid_date</th>\n",
       "      <th>scr</th>\n",
       "      <th>egfr_mdrd</th>\n",
       "      <th>dm</th>\n",
       "      <th>cvd</th>\n",
       "      <th>htn</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/data/put_data/timliu/kidney/pro_img/VBTEC/cla...</td>\n",
       "      <td>right</td>\n",
       "      <td>5377_I00068073416</td>\n",
       "      <td>5377</td>\n",
       "      <td>I00068073416</td>\n",
       "      <td>20080716</td>\n",
       "      <td>2008</td>\n",
       "      <td>5377_20080716</td>\n",
       "      <td>38.0</td>\n",
       "      <td>19.564004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/data/put_data/timliu/kidney/pro_img/VBTEC/cla...</td>\n",
       "      <td>right</td>\n",
       "      <td>5377_I00068073364</td>\n",
       "      <td>5377</td>\n",
       "      <td>I00068073364</td>\n",
       "      <td>20080716</td>\n",
       "      <td>2008</td>\n",
       "      <td>5377_20080716</td>\n",
       "      <td>38.0</td>\n",
       "      <td>19.564004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/data/put_data/timliu/kidney/pro_img/VBTEC/cla...</td>\n",
       "      <td>right</td>\n",
       "      <td>2014_I00104545140</td>\n",
       "      <td>2014</td>\n",
       "      <td>I00104545140</td>\n",
       "      <td>20120828</td>\n",
       "      <td>2012</td>\n",
       "      <td>2014_20120828</td>\n",
       "      <td>12.4</td>\n",
       "      <td>48.701118</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/data/put_data/timliu/kidney/pro_img/VBTEC/cla...</td>\n",
       "      <td>right</td>\n",
       "      <td>2014_I00104545135</td>\n",
       "      <td>2014</td>\n",
       "      <td>I00104545135</td>\n",
       "      <td>20120828</td>\n",
       "      <td>2012</td>\n",
       "      <td>2014_20120828</td>\n",
       "      <td>12.4</td>\n",
       "      <td>48.701118</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/data/put_data/timliu/kidney/pro_img/VBTEC/cla...</td>\n",
       "      <td>right</td>\n",
       "      <td>2014_I00104545139</td>\n",
       "      <td>2014</td>\n",
       "      <td>I00104545139</td>\n",
       "      <td>20120828</td>\n",
       "      <td>2012</td>\n",
       "      <td>2014_20120828</td>\n",
       "      <td>12.4</td>\n",
       "      <td>48.701118</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               path   type  \\\n",
       "0      0  /data/put_data/timliu/kidney/pro_img/VBTEC/cla...  right   \n",
       "1      1  /data/put_data/timliu/kidney/pro_img/VBTEC/cla...  right   \n",
       "2      2  /data/put_data/timliu/kidney/pro_img/VBTEC/cla...  right   \n",
       "3      3  /data/put_data/timliu/kidney/pro_img/VBTEC/cla...  right   \n",
       "4      4  /data/put_data/timliu/kidney/pro_img/VBTEC/cla...  right   \n",
       "\n",
       "                 uid  chtno          name  contentdate  year       uid_date  \\\n",
       "0  5377_I00068073416   5377  I00068073416     20080716  2008  5377_20080716   \n",
       "1  5377_I00068073364   5377  I00068073364     20080716  2008  5377_20080716   \n",
       "2  2014_I00104545140   2014  I00104545140     20120828  2012  2014_20120828   \n",
       "3  2014_I00104545135   2014  I00104545135     20120828  2012  2014_20120828   \n",
       "4  2014_I00104545139   2014  I00104545139     20120828  2012  2014_20120828   \n",
       "\n",
       "    scr  egfr_mdrd  dm  cvd  htn  sex  age  \n",
       "0  38.0  19.564004   0    0    0    0   33  \n",
       "1  38.0  19.564004   0    0    0    0   33  \n",
       "2  12.4  48.701118   0    1    1    1   45  \n",
       "3  12.4  48.701118   0    1    1    1   45  \n",
       "4  12.4  48.701118   0    1    1    1   45  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read full df\n",
    "df = pd.read_csv('/data/put_data/timliu/kidney/pro_img/VBTEC/clahe_right_label2/df_with_labels.csv', index_col = False)\n",
    "# df = pd.DataFrame([row for index, row in df.iterrows() if 'GE' in row['path']])\n",
    "# df2 = pd.read_csv('INFINITT_final_1.csv', index_col = False)\n",
    "# df3 = pd.read_csv('VBTEC_final.csv', index_col = False)\n",
    "# df = pd.concat((df, df2, df3), axis = 0)\n",
    "# df = df.reset_index(drop = True)\n",
    "df.scr = df.scr*10\n",
    "print(df.shape)\n",
    "# label classification\n",
    "# df['target'] = 0\n",
    "# for index, row in df.iterrows():\n",
    "#     eg = float(row['egfr_mdrd'])\n",
    "#     if eg >= 30:\n",
    "#         label = 0\n",
    "#     else:\n",
    "#         label = 1\n",
    "#     df.iloc[index, 16] = int(label)\n",
    "df = df[df.dm == 0].reset_index()\n",
    "# df_r = df[(df.length != 'Suspension') & (df.dm == 0)]\n",
    "# df_r.length = df_r.length.astype('float')\n",
    "\n",
    "# df_r = df_r[((df_r.egfr_mdrd >= 15) & (df_r.length <= 10)) | ((df_r.egfr_mdrd <= 90) & (df_r.length >= 8))]\n",
    "\n",
    "# df_r = df_r.reset_index()\n",
    "# df = df_r\n",
    "# print(df.shape)\n",
    "# df.length = df.length.astype('float')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for index, row in df.iterrows():\n",
    "#     df.iloc[index, 1] = '/data/put_data/timliu/kidney/pro_img/GE/' + 'removed_' + df.iloc[index, 1].split('/')[-2] + '/' + df.iloc[index, 1].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from statsmodels.distributions.empirical_distribution import ECDF\n",
    "# ecdf = ECDF(df.egfr_mdrd.values)\n",
    "# plt.plot(ecdf.x, ecdf.y)\n",
    "# plt.show()\n",
    "# plt.hist(df.egfr_mdrd.values, bins=50)\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unique_id = list(set([row['uid_date'] for index, row in df.iterrows()]))\n",
    "# one_kidney = []\n",
    "# for i in unique_id:\n",
    "#     df_test = df[df['uid_date'] == i]['type']\n",
    "#     if len(set(df_test)) < 2:\n",
    "#         one_kidney.append(i)\n",
    "        \n",
    "# df = pd.DataFrame([row for index, row in df.iterrows() if row['uid_date'] not in one_kidney])\n",
    "# df = df.reset_index(drop = True)\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all: 4685 train: 4197 test: 485\n"
     ]
    }
   ],
   "source": [
    "# split data every-step = 10 \n",
    "ratio = 0.9\n",
    "random.seed(45) #78\n",
    "train_id = []\n",
    "test_id = []\n",
    "for i in range(0, 34):\n",
    "    df_sub = df[df['egfr_mdrd'].between(5*i, 5*(i+1), inclusive = (True, False))]\n",
    "    d_list = Series.tolist(df_sub['uid_date'])\n",
    "    d_list = list(set(d_list))\n",
    "    random.shuffle(d_list)\n",
    "    train_id += d_list[:int(ratio*len(d_list))]\n",
    "    test_id += d_list[int(ratio*len(d_list)):]\n",
    "print('all:', len(set(df.egfr_mdrd)), 'train:', len(train_id), 'test:', len(test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting data \n",
    "# ratio = 0.8\n",
    "# unique_id = list(set(df['uid_date']))\n",
    "# random.shuffle(unique_id)\n",
    "# train_id = unique_id[:int(ratio*len(unique_id))]\n",
    "# test_id = unique_id[int(ratio*len(unique_id)):]\n",
    "# print(len(train_id))\n",
    "# print(len(test_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "# specify dtype\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "if use_cuda:\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    dtype = torch.FloatTensor\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 1e-6\n",
    "momentum = 0.9\n",
    "num_epoch = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class kidney_Dataset(Dataset):\n",
    "#     def __init__(self, data_tensor1, target_tensor):\n",
    "#         assert data_tensor1.size(0) ==  target_tensor.size(0)\n",
    "#         self.data_tensor1 = data_tensor1\n",
    "#         self.target_tensor = target_tensor\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return self.data_tensor1[index], self.target_tensor[index]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.data_tensor1.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_dataset = kidney_Dataset(torch.from_numpy(train_X), torch.from_numpy(train_Y))\n",
    "# test_dataset = kidney_Dataset(torch.from_numpy(test_X), torch.from_numpy(test_Y))\n",
    "# trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "# testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sometimes = lambda aug: iaa.Sometimes(0.9, aug)\n",
    "seq_1 = iaa.Sequential([\n",
    "    sometimes(iaa.Affine(\n",
    "        rotate=(-25, 25),\n",
    "        scale=(1, 1)\n",
    "    ))\n",
    "])\n",
    "\n",
    "seq_2 = iaa.Sequential([\n",
    "    iaa.Affine(rotate=(-10, 10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# center crop\n",
    "transformations_1 = transforms.Compose([transforms.CenterCrop([450, 450])])\n",
    "# normalize\n",
    "transformations_2 = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))]) # , transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_process_transform(img, augmentor, transformation, transform):\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    if transform:\n",
    "#         augmentor_det = augmentor.to_deterministic()\n",
    "        img = augmentor.augment_images(img)\n",
    "    img = np.squeeze(img, axis = 0)\n",
    "    img = Image.fromarray(img.astype('uint8'), mode=\"RGB\")\n",
    "    img = transformation(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from skimage.filters import gaussian\n",
    "size = 224, 224\n",
    "def img_process_PIL(path, transform):\n",
    "    # random room and central crop\n",
    "    img = cv2.imread(path) \n",
    "    b, g, r = cv2.split(img)\n",
    "    img = cv2.merge((r, g, b))\n",
    "    img = img_process_transform(img, seq_1, transformations_1, transform)\n",
    "    img.thumbnail(size, Image.ANTIALIAS)\n",
    "#     if transform:\n",
    "    img = img_process_transform(img, seq_2, transformations_2, transform)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# img = img_process_PIL(df['path'].values[1700], transform = False)\n",
    "# img\n",
    "# img.size\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class kidney_Dataset(Dataset):\n",
    "    def __init__(self, train_id, df, transform = True):\n",
    "        self.train_id = train_id\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        sample_count = len(self.df[self.df['uid_date'] == self.train_id[index]]['path'].values)\n",
    "        select_index = random.randint(0, sample_count-1)\n",
    "        path = self.df[self.df['uid_date'] == self.train_id[index]]['path'].values[select_index]\n",
    "        img = img_process_PIL(path, self.transform)\n",
    "#         length = self.df[self.df['uid_date'] == self.train_id[index]]['length'].values\n",
    "#         max_ind = np.where(length == np.max(length))[0][0]\n",
    "#         length = np.atleast_1d(length[max_ind])\n",
    "        sex = np.atleast_1d(self.df[self.df['uid_date'] == self.train_id[index]]['sex'].values[select_index])\n",
    "        age = np.atleast_1d((self.df[self.df['uid_date'] == self.train_id[index]]['age'].values[select_index] - np.mean(self.df.age)) / np.std(self.df.age))\n",
    "        cvd = np.atleast_1d(self.df[self.df['uid_date'] == self.train_id[index]]['cvd'].values[select_index])\n",
    "        htn = np.atleast_1d(self.df[self.df['uid_date'] == self.train_id[index]]['htn'].values[select_index])\n",
    "        label = self.df[self.df['uid_date'] == self.train_id[index]]['egfr_mdrd'].values[select_index]\n",
    "        return img,sex,age,cvd,htn,label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = kidney_Dataset(train_id, df, transform = False)\n",
    "test_dataset = kidney_Dataset(test_id, df, transform = False)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataiter = iter(trainloader)\n",
    "# images1,sex,age,cvd,htn, labels = dataiter.next()\n",
    "# print(images1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.base_model = models.vgg16(pretrained=True).type(dtype)\n",
    "#         self.left_model = nn.Sequential(*list(copy.deepcopy(self.base_model).features.children()))\n",
    "#         self.left_model.conv1 = nn.Conv2d(1, 64, kernel_size = (7, 7), stride=(2, 2), padding = (3,3), bias = False).type(dtype)\n",
    "#         self.left_model.features._modules['0'] = nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1), bias = False).type(dtype)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1004, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x1, sex, age, cvd, dm):\n",
    "        out2 = self.base_model(x1)\n",
    "        out2 = out2.view(out2.size(0), -1)\n",
    "        out = torch.cat((out2, sex, age, cvd, dm), 1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model = CNN().type(dtype)\n",
    "# model = models.resnet101(pretrained=False).type(dtype)\n",
    "# change input size\n",
    "# model.features._modules['0'] = nn.Conv2d(1, 64, kernel_size=(3,3), stride=(1,1), padding=(1,1)).type(dtype)\n",
    "# model.conv1 = nn.Conv2d(1, 64, kernel_size = (3, 3), stride=(2, 2), padding = (3,3), bias = False).type(dtype)\n",
    "# model.fc = nn.Linear(2048, 1)\n",
    "# model.classifier._modules['6'] = nn.Linear(4096, 5).type(dtype)\n",
    "# model.fc = nn.Sequential(nn.Linear(2048, 1)).type(dtype)\n",
    "# model.classifier._modules['7'] = nn.Dropout(p = 0.5).type(dtype)\n",
    "# model.classifier._modules['8'] = nn.Linear(1000, 1).type(dtype)\n",
    "net = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.classifier = nn.Sequential(nn.Linear(2208, 1000), \n",
    "#                                 nn.ReLU(),\n",
    "#                                 nn.Linear(1000, 1)\n",
    "#                                 ).type(dtype)\n",
    "# net = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # # # test\n",
    "# t1 = Variable(torch.rand(64, 3, 224, 224)).type(dtype)\n",
    "# lengtht = Variable(torch.rand(64, 1)).type(dtype)\n",
    "# sext = Variable(torch.rand(64, 1)).type(dtype)\n",
    "# aget = Variable(torch.rand(64, 1)).type(dtype)\n",
    "# dmt = Variable(torch.rand(64, 1)).type(dtype)\n",
    "# net(t1,lengtht,sext,aget,dmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_losses(loss_history1=None, loss_history2=None):\n",
    "    plt.clf()\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    if loss_history1:\n",
    "        ax1.plot(loss_history1, color=\"blue\", label=\"train\")\n",
    "    if loss_history2:\n",
    "        ax1.plot(loss_history2, color=\"orange\", label=\"test\")\n",
    "    #ax2 = ax1.twinx()\n",
    "    #ax2.set_yscale('log')\n",
    "    plt.xlabel(\"epoch\") \n",
    "    plt.ylabel(\"loss\") \n",
    "    plt.legend(bbox_to_anchor=(0.8, 0.9), loc=2, borderaxespad=0.)\n",
    "    plt.title(\"loss\")\n",
    "    plt.savefig(model_dir + 'output_losses.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model will be saved to  model_for_prediction/0306/rgb_clahe_vbtec_vgg16_single_removedm_removelabel/\n"
     ]
    }
   ],
   "source": [
    "# loss and optimizer\n",
    "# criterion = nn.SmoothL1Loss()\n",
    "criterion = nn.MSELoss(size_average=False)\n",
    "criterion2 = nn.L1Loss(size_average=False)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum, nesterov=True, weight_decay=1e-6)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=10,verbose=True)\n",
    "# model dir\n",
    "time_str = time.strftime(\"%m%d\")\n",
    "model_dir = 'model_for_prediction/' + time_str + '/rgb_clahe_vbtec_vgg16_single_removedm_removelabel/' \n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "print('model will be saved to ', model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i, tdata in enumerate(testloader, 0):\n",
    "#     tinputs1, tsex, tage, tcvd, tdm, tlabels = tdata\n",
    "#     tinputs1, tsex, tage, tcvd, tdm, tlabels = Variable(tinputs1, volatile=True).type(dtype), Variable(tsex, volatile=True).type(dtype), Variable(tage, volatile=True).type(dtype), Variable(tcvd, volatile=True).type(dtype), Variable(tdm, volatile=True).type(dtype), Variable(tlabels, volatile=True).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Train Loss: 1810.043, Test Loss: 2046.839, Test Loss l1: 33.791, Time: 46.018\n",
      "Epoch 2/300, Train Loss: 1358.428, Test Loss: 1058.109, Test Loss l1: 23.282, Time: 42.204\n",
      "Epoch 3/300, Train Loss: 724.286, Test Loss: 913.529, Test Loss l1: 22.866, Time: 42.328\n",
      "Epoch 4/300, Train Loss: 703.840, Test Loss: 888.829, Test Loss l1: 22.892, Time: 42.553\n",
      "Epoch 5/300, Train Loss: 699.078, Test Loss: 883.816, Test Loss l1: 22.641, Time: 42.638\n",
      "Epoch 6/300, Train Loss: 683.323, Test Loss: 877.188, Test Loss l1: 22.666, Time: 42.770\n",
      "Epoch 7/300, Train Loss: 673.381, Test Loss: 874.466, Test Loss l1: 22.507, Time: 42.234\n",
      "Epoch 8/300, Train Loss: 672.020, Test Loss: 858.858, Test Loss l1: 22.493, Time: 42.219\n",
      "Epoch 9/300, Train Loss: 662.368, Test Loss: 861.471, Test Loss l1: 22.458, Time: 42.802\n",
      "Epoch 10/300, Train Loss: 650.739, Test Loss: 845.958, Test Loss l1: 22.029, Time: 42.650\n",
      "Epoch 11/300, Train Loss: 642.737, Test Loss: 840.281, Test Loss l1: 22.158, Time: 42.792\n",
      "Epoch 12/300, Train Loss: 628.052, Test Loss: 813.185, Test Loss l1: 22.027, Time: 41.994\n",
      "Epoch 13/300, Train Loss: 620.771, Test Loss: 803.655, Test Loss l1: 21.597, Time: 42.668\n",
      "Epoch 14/300, Train Loss: 606.675, Test Loss: 803.839, Test Loss l1: 21.461, Time: 42.640\n",
      "Epoch 15/300, Train Loss: 588.802, Test Loss: 769.815, Test Loss l1: 21.544, Time: 42.429\n",
      "Epoch 16/300, Train Loss: 578.057, Test Loss: 749.655, Test Loss l1: 20.992, Time: 42.754\n",
      "Epoch 17/300, Train Loss: 563.318, Test Loss: 750.818, Test Loss l1: 20.759, Time: 42.368\n",
      "Epoch 18/300, Train Loss: 543.591, Test Loss: 706.587, Test Loss l1: 20.698, Time: 42.637\n",
      "Epoch 19/300, Train Loss: 541.457, Test Loss: 709.877, Test Loss l1: 20.694, Time: 42.402\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_loss_l1 = []\n",
    "test_loss = []\n",
    "patience = 0\n",
    "best_loss = float('Inf')\n",
    "\n",
    "# start training\n",
    "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
    "    running_loss1 = []\n",
    "    running_loss2 = []\n",
    "    running_loss3 = []\n",
    "    tStart = time.time()\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        net.train(True)    \n",
    "        # get the inputs\n",
    "        inputs1, sex, age, cvd, dm, labels = data\n",
    "        # wrap them in Variable\n",
    "        inputs1, sex, age, cvd, dm, labels = Variable(inputs1).type(dtype),\\\n",
    "                                             Variable(sex).type(dtype),\\\n",
    "                                             Variable(age).type(dtype),\\\n",
    "                                             Variable(cvd).type(dtype),\\\n",
    "                                             Variable(dm).type(dtype),\\\n",
    "                                             Variable(labels).type(dtype)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs1, sex, age, cvd, dm)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        # loss\n",
    "        running_loss1.append(loss.data[0])\n",
    "        \n",
    "    for i, tdata in enumerate(testloader, 0):\n",
    "        net.train(False)\n",
    "        tinputs1, tsex, tage, tcvd, tdm, tlabels = tdata\n",
    "        tinputs1, tsex, tage, tcvd, tdm, tlabels = Variable(tinputs1, volatile=True).type(dtype),\\\n",
    "                                                   Variable(tsex, volatile=True).type(dtype),\\\n",
    "                                                   Variable(tage, volatile=True).type(dtype),\\\n",
    "                                                   Variable(tcvd, volatile=True).type(dtype),\\\n",
    "                                                   Variable(tdm, volatile=True).type(dtype),\\\n",
    "                                                   Variable(tlabels, volatile=True).type(dtype) \n",
    "        toutput = net(tinputs1, tsex, tage, tcvd, tdm)\n",
    "        tloss = criterion(toutput, tlabels)\n",
    "        tloss_l1 = criterion2(toutput, tlabels)\n",
    "        running_loss2.append(tloss.data[0])\n",
    "        running_loss3.append(tloss_l1.data[0])\n",
    "    \n",
    "    train_loss_epoch, test_loss_epoch, test_loss_epoch_l1 = np.sum(running_loss1) / len(train_id),\\\n",
    "                                                            np.sum(running_loss2) / len(test_id),\\\n",
    "                                                            np.sum(running_loss3) / len(test_id)\n",
    "    scheduler.step(test_loss_epoch)\n",
    "    \n",
    "    train_loss.append(train_loss_epoch)\n",
    "    test_loss.append(test_loss_epoch)\n",
    "    test_loss_l1.append(test_loss_epoch_l1)\n",
    "    \n",
    "    tEnd = time.time()\n",
    "    \n",
    "    #statistics\n",
    "    print('Epoch {}/{}, Train Loss: {:.3f}, Test Loss: {:.3f}, Test Loss l1: {:.3f}, Time: {:.3f}'.format(epoch+1, num_epoch, train_loss[-1], test_loss[-1], test_loss_l1[-1],(tEnd-tStart)))\n",
    "\n",
    "\n",
    "    #early stopping\n",
    "    if epoch > 20:\n",
    "        if test_loss[-1] < best_loss:\n",
    "            best_loss = test_loss[-1]\n",
    "            torch.save(net.state_dict(), model_dir + 'model.pkl')\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            \n",
    "        if patience > 50:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "        # loss history \n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        plot_losses(train_loss, test_loss)\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model \n",
    "net.load_state_dict(torch.load(model_dir+ 'model.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_losses(train_loss, test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def class_generator(dataloader, net):\n",
    "#     true = []\n",
    "#     pred = []\n",
    "#     for i, data in enumerate(dataloader):\n",
    "#         sample, label = data\n",
    "#         sample, label = Variable(sample, volatile = True).type(dtype), Variable(label, volatile = True).type(torch.cuda.LongTensor)\n",
    "#         output = net(sample)\n",
    "#         _, predicted = torch.max(output.data, 1)\n",
    "#         true.extend(label.data.cpu().numpy())\n",
    "#         pred.extend(predicted.cpu().numpy())\n",
    "#     true = np.asarray(true)\n",
    "#     pred = np.asarray(pred)\n",
    "#     return true, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# true, pred = class_generator(testloader, net.eval())\n",
    "# true = np.squeeze(true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def plot_confusion_matrix(cm, classes,\n",
    "#                           normalize=False,\n",
    "#                           title='Confusion matrix',\n",
    "#                           cmap=plt.cm.Blues):\n",
    "#     if normalize:\n",
    "#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=45)\n",
    "#     plt.yticks(tick_marks, classes)\n",
    "\n",
    "#     fmt = '.2f' if normalize else 'd'\n",
    "#     thresh = cm.max() / 2.\n",
    "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#         plt.text(j, i, format(cm[i, j], fmt),\n",
    "#                  horizontalalignment=\"center\",\n",
    "#                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "# import itertools\n",
    "# print('Accuracy: {:.3f}%'.format(metrics.accuracy_score(true, pred)*100))\n",
    "# unique, counts = np.unique(true, return_counts=True)\n",
    "# print('Counts of each labels: ', dict(zip(unique, counts)))\n",
    "# cm = metrics.confusion_matrix(true, pred)\n",
    "# plot_confusion_matrix(cm, classes = np.sort(list(set(true))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.metrics import mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_making(true, pred, types):\n",
    "    cor = pearsonr(true, pred)[0]\n",
    "    mae = mean_absolute_error(true, pred)\n",
    "    r2 = r2_score(true, pred) \n",
    "    plt.figure(0)\n",
    "    plt.scatter(true, pred, alpha = .15, s = 20)\n",
    "    plt.xlabel('True_Y')\n",
    "    plt.ylabel('Pred_Y')\n",
    "    plt.title(\" data \\n\" + \"MAE = %4f; Cor = %4f; R2 = %4f; #samples = %d\" % (mae, cor, r2, len(true)))\n",
    "    plt.savefig(model_dir + types + \"_plot_scatter.png\" , dpi = 200)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def loss_generator(testloader, model, types):\n",
    "    test_loss = []\n",
    "    true = []\n",
    "    pred = []\n",
    "    for i, tdata in enumerate(testloader, 0):\n",
    "        tinputs1, tsex, tage, tcvd, tdm, tlabels = tdata\n",
    "        tinputs1, tsex, tage, tcvd, tdm, tlabels = Variable(tinputs1, volatile=True).type(dtype), Variable(tsex, volatile=True).type(dtype), Variable(tage, volatile=True).type(dtype), Variable(tcvd, volatile=True).type(dtype), Variable(tdm, volatile=True).type(dtype), Variable(tlabels, volatile=True).type(dtype)\n",
    "        toutput = net(tinputs1, tsex, tage, tcvd, tdm)\n",
    "        tloss = criterion(toutput, tlabels)\n",
    "        test_loss.append(tloss.data[0])\n",
    "        true.extend(tlabels.data.cpu().numpy())\n",
    "        pred.extend(toutput.data.cpu().numpy())\n",
    "    true = np.asarray(true)\n",
    "    true = np.expand_dims(true, axis=1)\n",
    "    pred = np.asarray(pred)\n",
    "    plot_making(true, pred, types = types)\n",
    "    print('L1 Loss on images: %r' % (np.sum(test_loss)/pred.shape[0]))\n",
    "    \n",
    "train_dataset = kidney_Dataset(train_id, df, transform = False)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "loss_generator(testloader, net.eval(), types = 'test')\n",
    "loss_generator(trainloader, net.eval(), types = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
