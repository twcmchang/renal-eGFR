{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load model.py\n",
    "import inspect\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "\n",
    "class VGG16:\n",
    "    def __init__(self, vgg16_npy_path, classes=1, shape=(224,224,3)):\n",
    "        \"\"\"\n",
    "        load pre-trained weights from path\n",
    "        :param vgg16_npy_path: file path of vgg16 pre-trained weights\n",
    "        \"\"\"\n",
    "\n",
    "        # input information\n",
    "        self.H, self.W, self.C = shape\n",
    "        self.classes = classes\n",
    "        self.data_dict = np.load(vgg16_npy_path, encoding='latin1').item()\n",
    "        print(\"npy file loaded\")\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        load variable from npy to build the VGG\n",
    "        :param rgb: rgb image [batch, height, width, 3] values scaled [0, 1]\n",
    "        \"\"\"\n",
    "\n",
    "        # input placeholder\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.H, self.W, self.C])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.classes])\n",
    "        self.w = tf.placeholder(tf.float32, [None, self.classes])\n",
    "        \n",
    "        rgb_scaled = self.x\n",
    "\n",
    "        # normalize input by VGG_MEAN\n",
    "        red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=rgb_scaled)\n",
    "        assert   red.get_shape().as_list()[1:] == [self.H, self.W, 1]\n",
    "        assert green.get_shape().as_list()[1:] == [self.H, self.W, 1]\n",
    "        assert  blue.get_shape().as_list()[1:] == [self.H, self.W, 1]\n",
    "\n",
    "        self.x = tf.concat(axis=3, values=[\n",
    "              blue - VGG_MEAN[0],\n",
    "             green - VGG_MEAN[1],\n",
    "               red - VGG_MEAN[2],\n",
    "        ])\n",
    "        assert self.x.get_shape().as_list()[1:] == [self.H, self.W, self.C]\n",
    "\n",
    "        start_time = time.time()\n",
    "        print(\"build model started\")\n",
    "\n",
    "        assert self.x.get_shape().as_list()[1:] == [224, 224, 3]\n",
    "\n",
    "        self.conv1_1 = self.conv_layer(self.x, \"conv1_1\")\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, \"conv1_2\")\n",
    "        self.pool1 = self.max_pool(self.conv1_2, 'pool1')\n",
    "\n",
    "        self.conv2_1 = self.conv_layer(self.pool1, \"conv2_1\")\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, \"conv2_2\")\n",
    "        self.pool2 = self.max_pool(self.conv2_2, 'pool2')\n",
    "\n",
    "        self.conv3_1 = self.conv_layer(self.pool2, \"conv3_1\")\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, \"conv3_2\")\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, \"conv3_3\")\n",
    "        self.pool3 = self.max_pool(self.conv3_3, 'pool3')\n",
    "\n",
    "        self.conv4_1 = self.conv_layer(self.pool3, \"conv4_1\")\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, \"conv4_2\")\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, \"conv4_3\")\n",
    "        self.pool4 = self.max_pool(self.conv4_3, 'pool4')\n",
    "\n",
    "        self.conv5_1 = self.conv_layer(self.pool4, \"conv5_1\")\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, \"conv5_2\")\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, \"conv5_3\")\n",
    "        self.pool5 = self.max_pool(self.conv5_3, 'pool5')\n",
    "\n",
    "        self.flatten_input = self.flatten(self.pool5)\n",
    "\n",
    "        self.W1 = tf.get_variable(shape=(self.flatten_input.shape[1],1024), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), name=\"W1\", dtype=tf.float32)\n",
    "        self.b1 = tf.get_variable(shape=1024, initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), name=\"b1\", dtype=tf.float32)\n",
    "        fc1 = tf.nn.bias_add(tf.matmul(self.flatten_input, self.W1), self.b1)\n",
    "        self.fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "        self.W2 = tf.get_variable(shape=(1024,256), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), name=\"W2\", dtype=tf.float32)\n",
    "        self.b2 = tf.get_variable(shape=256, initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), name=\"b2\", dtype=tf.float32)\n",
    "        fc2 = tf.nn.bias_add(tf.matmul(self.fc1, self.W2), self.b2)\n",
    "        self.fc2 = tf.nn.relu(fc2)\n",
    "\n",
    "        self.W3 = tf.get_variable(shape=(256,1), initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), name=\"W3\", dtype=tf.float32)\n",
    "        self.b3 = tf.get_variable(shape=1, initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), name=\"b3\", dtype=tf.float32)\n",
    "        self.output = tf.nn.bias_add(tf.matmul(self.fc2, self.W3), self.b3)\n",
    "\n",
    "        self.loss = tf.losses.mean_squared_error(labels=self.y, predictions=self.output, weights=self.w)\n",
    "\n",
    "        print((\"build model finished: %ds\" % (time.time() - start_time)))\n",
    "\n",
    "    def avg_pool(self, bottom, name):\n",
    "        return tf.nn.avg_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    def max_pool(self, bottom, name):\n",
    "        return tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    def flatten(self,bottom):\n",
    "        shape = bottom.get_shape().as_list()\n",
    "        dim = 1\n",
    "        for d in shape[1:]:\n",
    "            dim *= d\n",
    "        x = tf.reshape(bottom, [-1, dim])\n",
    "        return x\n",
    "\n",
    "    def conv_layer(self, bottom, name):\n",
    "        with tf.variable_scope(name):\n",
    "            filt = self.get_conv_filter(name)\n",
    "\n",
    "            conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "            conv_biases = self.get_bias(name)\n",
    "            bias = tf.nn.bias_add(conv, conv_biases)\n",
    "\n",
    "            relu = tf.nn.relu(bias)\n",
    "            return relu\n",
    "\n",
    "    def get_conv_filter(self, name):\n",
    "        return tf.get_variable(initializer=self.data_dict[name][0], name=\"filter\")\n",
    "\n",
    "    def get_bias(self, name):\n",
    "        return tf.get_variable(initializer=self.data_dict[name][1], name=\"biases\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from progress.bar import Bar\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,4\"\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from model import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_file = '/data/put_data/timliu/kidney/pro_img/GE/meta.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydirs = ['/data/put_data/timliu/kidney/pro_img/GE/clahe'+str(i)+'_right' for i in range(5,26,5)]+['/data/put_data/timliu/kidney/pro_img/GE/clahe'+str(i)+'_left' for i in range(5,26,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori = pd.read_csv(original_file, index_col = False)\n",
    "df = ori.copy()\n",
    "if len(mydirs)>0:\n",
    "    for dirn in mydirs:\n",
    "        tmp = pd.read_csv(dirn+'/df_with_labels.csv', index_col = False)\n",
    "        print(tmp.shape)\n",
    "        df = df.append(tmp)\n",
    "\n",
    "df = df[(df.length != 'Suspension')].reset_index()\n",
    "\n",
    "ori = ori[(ori.length != 'Suspension')].reset_index()\n",
    "\n",
    "df.length = df.length.astype('float')\n",
    "print(df.shape)\n",
    "print(ori.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.75\n",
    "random.seed(45)\n",
    "train_id = []\n",
    "test_id = []\n",
    "for i in range(0, 34):\n",
    "    df_sub = df[df['egfr_mdrd'].between(5*i, 5*(i+1), inclusive = (True, False))]\n",
    "    d_list = pd.Series.tolist(df_sub['uid_date'])\n",
    "    d_list = list(set(d_list))\n",
    "    random.shuffle(d_list)\n",
    "    train_id += d_list[:int(ratio*len(d_list))]\n",
    "    test_id += d_list[int(ratio*len(d_list)):]\n",
    "print('all:', len(set(df.uid_date)), 'train:', len(train_id), 'test:', len(test_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, uid, df, crop_size, resize_size):\n",
    "        index = np.where(df['uid_date'].isin(uid))[0]\n",
    "        images = []\n",
    "        labels = []\n",
    "        weights = []\n",
    "        for i in index:\n",
    "            img = self._load_image(df['path'][i], crop_size, resize_size)\n",
    "            label = df['egfr_mdrd'][i]\n",
    "            if label < 30:\n",
    "                weight = 3\n",
    "            elif label < 60:\n",
    "                weight = 1\n",
    "            elif label < 90:\n",
    "                weight = 3\n",
    "            else:\n",
    "                weight = 9\n",
    "            images.append(img)\n",
    "            labels.append([label])\n",
    "            weights.append([weight])\n",
    "        self.images = np.array(images)\n",
    "        self.labels = np.array(labels)\n",
    "        self.weights = np.array(weights)\n",
    "    \n",
    "    def _load_image(self, path, crop_size, resize_size):\n",
    "        # load image\n",
    "        img = cv2.imread(path,1)\n",
    "        # we crop image from center\n",
    "        yy = int((img.shape[0] - crop_size) / 2)\n",
    "        xx = int((img.shape[1] - crop_size) / 2)\n",
    "        crop_img = img[yy: yy + crop_size, xx: xx + crop_size]\n",
    "        # resize to 224, 224\n",
    "        resized_img = cv2.resize(crop_img,(resize_size,resize_size),interpolation=cv2.INTER_AREA)\n",
    "        return resized_img\n",
    "    \n",
    "    def shuffle(self):\n",
    "        idx = np.random.permutation(self.images.shape[0])\n",
    "        self.images = self.images[idx,:,:,:]\n",
    "        self.labels = self.labels[idx]\n",
    "        self.weights = self.weights[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16_train(model, train, test, init_from, save_dir, batch_size=64, epoch=300, early_stop_patience=25):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    checkpoint_path = os.path.join(save_dir, 'model.ckpt')\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print(tf.trainable_variables())\n",
    "        \n",
    "        # hyper parameters\n",
    "        learning_rate =  5e-4 #adam\n",
    "        min_delta = 0.0001\n",
    "\n",
    "        # recorder\n",
    "        epoch_counter = 0\n",
    "        loss_history = []\n",
    "        val_loss_history = []\n",
    "\n",
    "        # optimizer\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = opt.minimize(model.loss)\n",
    "        \n",
    "        # saver \n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=2)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # progress bar\n",
    "        ptrain = IntProgress()\n",
    "        pval = IntProgress()\n",
    "        display(ptrain)\n",
    "        display(pval)\n",
    "        ptrain.max = int(train.images.shape[0]/batch_size)\n",
    "        pval.max = int(test.images.shape[0]/batch_size)\n",
    "\n",
    "        # reset due to adding a new task\n",
    "        patience_counter = 0\n",
    "        current_best_val_loss = 100000 # a large number\n",
    "        \n",
    "\n",
    "        # train start\n",
    "        while(patience_counter < early_stop_patience):\n",
    "            stime = time.time()\n",
    "            bar_train = Bar('Training', max=int(train.images.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "            bar_val =  Bar('Validation', max=int(test.images.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "            \n",
    "            # training an epoch\n",
    "            train_loss = 0\n",
    "            for i in range(int(train.images.shape[0]/batch_size)):\n",
    "                st = i*batch_size\n",
    "                ed = (i+1)*batch_size\n",
    "                \n",
    "                _, loss = sess.run([train_op, model.loss],\n",
    "                                   feed_dict={model.x: train.images[st:ed,:],\n",
    "                                              model.y: train.labels[st:ed,:],\n",
    "                                              model.w: train.weights[st:ed,:]\n",
    "                                             })\n",
    "                train_loss += loss\n",
    "                ptrain.value +=1\n",
    "                ptrain.description = \"Training %s/%s\" % (i, ptrain.max)\n",
    "                bar_train.next()\n",
    "            \n",
    "            train_loss /= ptrain.max\n",
    "            \n",
    "            val_loss = 0\n",
    "\n",
    "            for i in range(int(test.images.shape[0]/batch_size)):\n",
    "                st = i*batch_size\n",
    "                ed = (i+1)*batch_size\n",
    "                \n",
    "                loss = sess.run(model.loss,\n",
    "                                   feed_dict={model.x: test.images[st:ed,:],\n",
    "                                              model.y: test.labels[st:ed,:],\n",
    "                                              model.w: np.expand_dims(np.repeat(1.0,batch_size),axis=1)\n",
    "                                             })\n",
    "                val_loss += loss\n",
    "                pval.value +=1\n",
    "                pval.description = \"Training %s/%s\" % (i, pval.max)\n",
    "                bar_val.next()\n",
    "                \n",
    "            val_loss /= pval.max\n",
    "            \n",
    "            if (current_best_val_loss - val_loss) > min_delta:\n",
    "                current_best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                saver.save(sess, checkpoint_path, global_step=epoch_counter)\n",
    "                print(\"reset early stopping and save model into %s at epoch %s\" % (checkpoint_path,epoch_counter))\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            # shuffle Xtrain and Ytrain in the next epoch\n",
    "            train.shuffle()\n",
    "            \n",
    "            loss_history.append(train_loss)\n",
    "            val_loss_history.append(val_loss)\n",
    "\n",
    "            ptrain.value = 0\n",
    "            pval.value = 0\n",
    "            bar_train.finish()\n",
    "            bar_val.finish()\n",
    "            print(\"Epoch %s (%s), %s sec >> train-loss: %.4f, val-loss: %.4f\" % (epoch_counter, patience_counter, round(time.time()-stime,2), train_loss, val_loss))\n",
    "            \n",
    "            # epoch end\n",
    "            epoch_counter += 1\n",
    "            if epoch_counter >= epoch:\n",
    "                break\n",
    "        res = pd.DataFrame({\"epoch\":range(0,len(loss_history)), \"loss\":loss_history, \"val_loss\":val_loss_history})\n",
    "        res.to_csv(os.path.join(save_dir,\"history.csv\"), index=False)\n",
    "        print(\"end training\")\n",
    "\n",
    "def plot_making(save_dir, true, pred, types):\n",
    "    from scipy.stats.stats import pearsonr\n",
    "    from sklearn.metrics import mean_absolute_error, r2_score\n",
    "    cor = pearsonr(true, pred)[0]\n",
    "    mae = mean_absolute_error(true, pred)\n",
    "    r2 = r2_score(true, pred) \n",
    "    plt.figure(0)\n",
    "    plt.scatter(true, pred, alpha = .15, s = 20)\n",
    "    plt.xlabel('True_Y')\n",
    "    plt.ylabel('Pred_Y')\n",
    "    plt.title(\" data \\n\" + \"MAE = %4f; Cor = %4f; R2 = %4f; #samples = %d\" % (mae, cor, r2, len(true)))\n",
    "    plt.savefig(save_dir + types + \"_plot_scatter.png\" , dpi = 200)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    hist = pd.read_csv(os.path.join(save_dir, \"history.csv\"))\n",
    "    \n",
    "    plt.figure(0)\n",
    "    hist.plot(x='epoch', markevery=5)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.savefig(save_dir+\"history.png\",dpi=200)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def save_plot(model, save_dir, test, batch_size=64):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "        pred = []\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            sess.run(tf.global_variables())\n",
    "            for i in range(test.images.shape[0]):\n",
    "                output = sess.run(model.output,\n",
    "                                   feed_dict={model.x: test.images[i:(i+1),:],\n",
    "                                              model.y: test.labels[i:(i+1),:],\n",
    "                                              model.w: [[1.0]]\n",
    "                                             })\n",
    "\n",
    "                pred.append(output)\n",
    "            pred = np.reshape(pred,newshape=(-1,1))\n",
    "            plot_making(save_dir, test.labels, pred, types=\"test\")\n",
    "            \n",
    "            print(\"mean square error: %.4f\" % np.mean(np.square(test.labels-pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crop_size = 350\n",
    "resize_size = 224\n",
    "train = Dataset(train_id, df, crop_size, resize_size)\n",
    "test  = Dataset(test_id, ori, crop_size, resize_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training dataset: \", train.images.shape)\n",
    "print(\"test dataset: \", test.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_dir = \"tf_model\"\n",
    "init_from = \"vgg16.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.variable_scope(\"test\") as scope:\n",
    "vgg16 = VGG16(vgg16_npy_path=init_from)\n",
    "vgg16.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_train(vgg16, train, test, save_dir=save_dir, init_from=init_from, batch_size=64, epoch=1, early_stop_patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot(vgg16, save_dir, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
